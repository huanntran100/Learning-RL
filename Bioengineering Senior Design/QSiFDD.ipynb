{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QSiFDD (Q-Learning Sparse incremental Feature Dependency Discovery)\n",
    "All \"helper\" methods below are required to run Q-Learning and Sparse iFDD. At the very bottom is the \"main\" method for running experimental batches (i.e. 5 trials of Q-learning vs. Q-learning w/ Sparse iFDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-06 18:25:14,328] Making new env: Swimmer-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Features: [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5] \n",
      "Action Features: [-0.036, 0.0, 0.036] \n",
      "Feature Map: [[-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5], [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5], [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5], [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5], [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5], [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5], [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5], [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5], [-0.036, 0.0, 0.036], [-0.036, 0.0, 0.036]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from itertools import combinations\n",
    "\n",
    "# Setting up the Swimmer-v1 environment\n",
    "env = gym.make('Swimmer-v1')\n",
    "\n",
    "# Hyperparameters and features\n",
    "STATE_LEN = 8\n",
    "ACTION_LEN = 2 \n",
    "OBSERVATION_SIZE = STATE_LEN + ACTION_LEN\n",
    "STATE_FEATURES = [-5, -4, -3, -2, -1, 0, -1, -2, -3, -4, -5]\n",
    "ACTION_FEATURES = [-0.036, 0., 0.036]\n",
    "POSSIBLE_ACTIONS = [-0.036, 0, 0.036]\n",
    "FEATURE_MAP = []\n",
    "for i in range(OBSERVATION_SIZE):\n",
    "    if i < STATE_LEN: FEATURE_MAP.append(STATE_FEATURES)\n",
    "    else: FEATURE_MAP.append(ACTION_FEATURES)\n",
    "print('State Features:', STATE_FEATURES, '\\nAction Features:', ACTION_FEATURES, '\\nFeature Map:', FEATURE_MAP)\n",
    "EPSILON = 0.01 # epsilon-greedy \n",
    "THRESHOLD = 0.9\n",
    "GAMMA = 0.001 # probability of discovering new feature if exceed threshold\n",
    "THETA = 0.0001 # probability of adding new feature to undiscovered set\n",
    "DISCOUNT_FACTOR = 0.001 # Q-learning\n",
    "LEARNING_RATE = 0.001 # weight TD update \n",
    "# ADDITIONAL_FEATURE_INDEX = len([item for sublist in FEATURE_MAP for item in sublist]) # number of current features \n",
    "# print('The first new feature is added at this index:', ADDITIONAL_FEATURE_INDEX)\n",
    "\n",
    "\n",
    "# Output: random state and random action\n",
    "def generateObservation():\n",
    "    env.reset()\n",
    "    for episode in range(1):\n",
    "        randomEpisode = np.random.randint(100)\n",
    "        for timeSteps in range(randomEpisode):\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "    testState = observation\n",
    "    testAction = randomAction()\n",
    "    print('Sample Observation:', testState, '\\nSample Action:', testAction)\n",
    "    return testState, testAction\n",
    "\n",
    "# Output: random action\n",
    "def randomAction():\n",
    "    global ACTION_LEN\n",
    "    randomAction = []\n",
    "    for i in range(ACTION_LEN):\n",
    "        randomIndex = np.random.randint(len(POSSIBLE_ACTIONS))\n",
    "        randomAction.append(POSSIBLE_ACTIONS[randomIndex])\n",
    "    return randomAction\n",
    "\n",
    "# Output: random state\n",
    "def randomState():\n",
    "    global STATE_LEN\n",
    "    randomState = []\n",
    "    for i in range(STATE_LEN):\n",
    "        randomIndex = np.random.randint(min(STATE_FEATURES), max(STATE_FEATURES))\n",
    "        randomState.append(STATE_FEATURES[randomIndex])\n",
    "    return randomState\n",
    "    \n",
    "\n",
    "# Output: randomly generated observation \n",
    "def generateRandomStateActionPair():\n",
    "    state = randomState()\n",
    "    action = randomAction()\n",
    "    return state, action\n",
    "\n",
    "# Input: state, action\n",
    "# Output: states + action observation rounded to nearest whole number (discretization)\n",
    "def observationProcessor(state, action):\n",
    "    global STATE_FEATURES\n",
    "    processedState = []\n",
    "    for i in range(len(state)): \n",
    "        processedState.append(int(state[i]))\n",
    "        if state[i] > max(STATE_FEATURES): processedState[i] = max(STATE_FEATURES)\n",
    "        elif state[i] < min(STATE_FEATURES): processedState[i] = min(STATE_FEATURES)\n",
    "    return processedState + action\n",
    "\n",
    "# Input: processed observation\n",
    "# Output: flattened basis function  and indexes \n",
    "def basisFunction(observation):\n",
    "    global OBSERVATION_SIZE, STATE_FEATURES, ACTION_FEATURES\n",
    "    activeFeature = []\n",
    "    for i in range(OBSERVATION_SIZE):\n",
    "        index = FEATURE_MAP[i].index(observation[i])\n",
    "        if i < STATE_LEN:\n",
    "            basisRow = np.zeros(len(STATE_FEATURES))\n",
    "            basisRow[index]  = 1\n",
    "            activeFeature.append(list(basisRow))\n",
    "        else:\n",
    "            basisRow = np.zeros(len(ACTION_FEATURES))\n",
    "            basisRow[index] = 1\n",
    "            activeFeature.append(list(basisRow))\n",
    "    # Flatten active feature  \n",
    "    activeFeature = [item for sublist in activeFeature for item in sublist]\n",
    "    # Get all indexes that are 1\n",
    "    activeIndex = [i for i, item in enumerate(activeFeature) if item == 1]\n",
    "    return activeFeature, activeIndex\n",
    "\n",
    "\n",
    "# Input: list of active indexes\n",
    "# Output: powerset containing all subsets of active indexes as such: feature, relevance\n",
    "def getPowerSet(activeIndexes):\n",
    "    powerSet = []\n",
    "    # Initialize relevance values  \n",
    "    initialRelevance = 0\n",
    "    # +1 to include the complete set as a set\n",
    "    for i in range(len(activeIndexes) + 1):\n",
    "        conjunctions = [[set(combo), initialRelevance] for combo in itertools.combinations(activeIndexes, i)]\n",
    "        powerSet.append(conjunctions)\n",
    "    # Flatten powerset into 1-dim list\n",
    "    powerSet = [item for sublist in powerSet for item in sublist] \n",
    "    return powerSet\n",
    "\n",
    "# Input: list of initial active indexes \n",
    "# Output: powerset containing \n",
    "def getInitialDiscoveredUndiscovered(initialActiveIndexes):\n",
    "    # Generate initial discovered set\n",
    "    discovered = dict()\n",
    "    for i in range(len(initialActiveIndexes)):\n",
    "        key = str(i)\n",
    "        discovered.update({key : set([initialActiveIndexes[i]])})\n",
    "    # Generate initial undiscovered set\n",
    "    initialPowerSet = getPowerSet(initialActiveIndexes)\n",
    "    # Undiscovered set does not include the size 1 initial features or the empty set, only conjunctions. \n",
    "    undiscovered =  [item for i, item in enumerate(initialPowerSet) if len(item[0]) > 1]    \n",
    "    return discovered, undiscovered\n",
    "\n",
    "# Input: set\n",
    "# Output: list of subsets\n",
    "def subset(mySet):\n",
    "    listSubset = []\n",
    "    for i in range(len(mySet) + 1):\n",
    "        listSubset.append(set(itertools.combinations(mySet, i)))\n",
    "    listSubset = [item for sublist in listSubset for item in sublist] \n",
    "    return listSubset\n",
    "\n",
    "\n",
    "# Input: active indexes and current basis function \n",
    "# Output: temp basis function for checking if all features active \n",
    "def getBasisFromActiveIndex(activeIndex, basisFunction):\n",
    "    tempBasis = np.zeros(len(basisFunction))\n",
    "    for i in range(len(activeIndex)):\n",
    "        tempBasis[activeIndex[i]] = 1.0\n",
    "    return tempBasis\n",
    "\n",
    "# Input: active indexes, current basis function, discovered\n",
    "# Output: temp basis function with subsets = 0 and conjunction = 1\n",
    "def activateConjunction(activeIndex, basisFunction, conjunctionKey):\n",
    "    for i in range(len(activeIndex)):\n",
    "        basisFunction[activeIndex[i]] = 0.0\n",
    "    activeConjunctionIndex = int(conjunctionKey)\n",
    "    basisFunction[activeConjunctionIndex] = 1.0\n",
    "    return basisFunction \n",
    " \n",
    "# Input: discovered set, basis function\n",
    "# Output: modified basis function \n",
    "def conjunctionBasisFunction(discoveredFeatures, basisFunction): \n",
    "    for key, feature in discoveredFeatures.items():\n",
    "        if len(feature) > 1: \n",
    "            featureSubsets = subset(feature)\n",
    "            for j in range(len(featureSubsets)):\n",
    "                activeIndex = list(featureSubsets[j])\n",
    "                tempBasis = getBasisFromActiveIndex(activeIndex, basisFunction)\n",
    "                if np.array_equal(tempBasis, basisFunction):\n",
    "                    basisFunction = activateConjunction(activeIndex, basisFunction, key)              \n",
    "    return basisFunction\n",
    "\n",
    "# Input: Q-values\n",
    "# Output: action, action index \n",
    "def epsilonGreedy(qValues):\n",
    "    global EPSILON, POSSIBLE_ACTIONS\n",
    "    randomNumber = np.random.uniform(0, 1)\n",
    "    if randomNumber < EPSILON:\n",
    "        actionIndex = np.argmax(qValues)\n",
    "        return POSSIBLE_ACTIONS[actionIndex], actionIndex\n",
    "    else: \n",
    "        actionIndex = np.random.randint(len(qValues))\n",
    "        return POSSIBLE_ACTIONS[actionIndex], actionIndex\n",
    "\n",
    "# Input: Q-values\n",
    "# Output: action vector \n",
    "def jointEpsilonGreedy(qValues):\n",
    "    jointAction = []\n",
    "    jointActionIndex = []\n",
    "    for i in range(ACTION_LEN):\n",
    "        action, actionIndex = epsilonGreedy(qValues)\n",
    "        jointAction.append(action)\n",
    "        jointActionIndex.append(actionIndex)\n",
    "    return jointAction, jointActionIndex\n",
    "\n",
    "# Sparse iFDD Algorithm\n",
    "# Input: td error, threshold, discovered features,  undiscovered features, additional feature index\n",
    "# Output: updated discovered and undiscovered sets\n",
    "def discover(tdError, threshold, discoveredFeatures, undiscoveredFeatures, additionalFeatureIndex, currentBasisFunction, weights): \n",
    "    global GAMMA, THETA \n",
    "    activeFeatures = [i for i, value in enumerate(currentBasisFunction) if value == 1.0]\n",
    "    # Get powerset of active features \n",
    "    current_PowerSet = getPowerSet(activeFeatures)\n",
    "    # Discover any features \n",
    "    activeOldFeatures = [i for i, feature in enumerate(undiscoveredFeatures) if feature in current_PowerSet and feature != set([])]\n",
    "    for i in range(len(activeOldFeatures)):\n",
    "        feature = undiscoveredFeatures[i]\n",
    "        feature[1] += tdError \n",
    "        # Discover the feature if exceed the threshold and delete from undiscoveredFeatures\n",
    "        # Discover feature with probability GAMMA  \n",
    "        randomNumber = np.random.uniform(0, 1)\n",
    "        if randomNumber < GAMMA and feature[1] > threshold and feature != set([]):\n",
    "            additionalFeatureIndex += 1\n",
    "            key = str(additionalFeatureIndex)\n",
    "            discoveredFeatures[key] = feature[0]\n",
    "            # Replace with undiscoverable relevance\n",
    "            # Update sizes of basis and weights\n",
    "            currentBasisFunction.append(0.0)\n",
    "            weights.append(np.random.uniform(low = -1, high = 1, size = 3))\n",
    "            # print('Feature:', feature, 'TDerror:', tdError, 'Feature Index:', additionalFeatureIndex)\n",
    "            undiscoveredFeatures[i][1] = -1e20\n",
    "                                                                              \n",
    "    # Add new features  \n",
    "    newUndiscoveredFeatures = [i for i, feature in enumerate(current_PowerSet) \n",
    "                               if feature[0] not in discoveredFeatures.values() \n",
    "                               and feature not in undiscoveredFeatures\n",
    "                               and feature != set([])]\n",
    "    for i in range(len(newUndiscoveredFeatures)): \n",
    "        feature = current_PowerSet[i]\n",
    "        randomNumber = np.random.uniform(0, 1)\n",
    "        if randomNumber < THETA:\n",
    "            undiscoveredFeatures.append(feature)           \n",
    "    return discoveredFeatures, undiscoveredFeatures, additionalFeatureIndex  \n",
    "\n",
    "# Input: weights, activeIndex, actionIndex, reward, oldQValue, newQValues, tdError\n",
    "# Output: weights \n",
    "def updateWeights(weights, activeIndex, actionIndex, reward, oldQValue, newQValues):\n",
    "    for i in range(len(activeIndex)): \n",
    "        tdError = reward + DISCOUNT_FACTOR * (max(newQValues) - oldQValue)  \n",
    "        weights[activeIndex[i]][actionIndex] += LEARNING_RATE * tdError\n",
    "    return weights, tdError\n",
    "\n",
    "# Input: initial basis function\n",
    "# Output: random weights \n",
    "def initializeWeights(basisFunction):\n",
    "    weightVector = []\n",
    "    for i in range(len(basisFunction)):\n",
    "        weightVector.append(np.random.uniform(low = -1, high = 1, size = 3))\n",
    "    return weightVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For running Q-Learning alone \n",
    "def QLearning(epsilon, episodeLen):\n",
    "    start_time = time.time()\n",
    "    TDErrorList1 = []\n",
    "    RewardList1 = []\n",
    "    totalReward1 = 0\n",
    "    EPSILON = epsilon\n",
    "    for episode in range(1):\n",
    "        env.reset()\n",
    "        # Start episode with random initial state and action   \n",
    "        initialState, initialAction = generateRandomStateActionPair()\n",
    "        # Process initial state and action\n",
    "        initialObservation = observationProcessor(initialState, initialAction)\n",
    "        # Get initial basis function and active indices \n",
    "        initialBasisFunction, initialActiveIndex = basisFunction(initialObservation)\n",
    "        print('Initial Active Features:', initialActiveIndex)\n",
    "        # Initialize weights \n",
    "        WEIGHT_VECTOR = initializeWeights(initialBasisFunction)\n",
    "        print('Len Initial Weight Vector:', len(WEIGHT_VECTOR))\n",
    "        # Divide into initial discovered and undiscovered feature sets\n",
    "        DISCOVERED, UNDISCOVERED = getInitialDiscoveredUndiscovered(initialActiveIndex)\n",
    "        print('Len Initial Discovered Set:', len(DISCOVERED), '\\nLen Initial Undiscovered Set:', len(UNDISCOVERED))\n",
    "        # Begin learning \n",
    "        oldObservation = initialObservation \n",
    "        # Multiply basis function and weights together\n",
    "        oldQValues = np.dot(initialBasisFunction, WEIGHT_VECTOR)\n",
    "        for timeSteps in range(episodeLen):\n",
    "            # Choose an action using epsilon-greedy\n",
    "            action, actionIndex = jointEpsilonGreedy(oldQValues)\n",
    "            # Get old Q value\n",
    "            oldQValue = 0.5 * (oldQValues[actionIndex[0]] + oldQValues[actionIndex[1]])\n",
    "            # Take a step in the environment \n",
    "            state, reward, done, info = env.step(action)\n",
    "            # Process observation\n",
    "            currentObservation = observationProcessor(state, action)\n",
    "            # Get basis function\n",
    "            currentBasisFunction, currentActiveIndex = basisFunction(currentObservation)\n",
    "            # Calculate new Q-values \n",
    "            currentQValues = np.dot(currentBasisFunction, WEIGHT_VECTOR)\n",
    "            # Update weights\n",
    "            WEIGHT_VECTOR, currentTDError = updateWeights(WEIGHT_VECTOR, \n",
    "                                          currentActiveIndex, \n",
    "                                          actionIndex, \n",
    "                                          reward, \n",
    "                                          oldQValue, \n",
    "                                          currentQValues)\n",
    "            # Logging \n",
    "            # Outputs information every 100000 timesteps\n",
    "            if timeSteps % 500000 == 0:\n",
    "                current_time = time.time()\n",
    "                print('Timestep:', timeSteps, \n",
    "                      'Time elapsed:', round(current_time - start_time, 2),\n",
    "                      'Cumulative Reward:', round(totalReward1, 4))\n",
    "\n",
    "            # Set current observation and Q-value to old\n",
    "            EPSILON += 0.00000001\n",
    "            totalReward1 += reward\n",
    "            TDErrorList1.append(currentTDError)\n",
    "            RewardList1.append(totalReward1)\n",
    "            oldObservation = currentObservation\n",
    "            oldQValues = currentQValues \n",
    "    end_time = time.time()\n",
    "    print('Training Time:', round(end_time - start_time, 2))\n",
    "    return TDErrorList1, RewardList1\n",
    "\n",
    "# Running Q-learning with initial state and action\n",
    "def QLearning2(epsilon, episodeLen, initState, initAction):\n",
    "    start_time = time.time()\n",
    "    TDErrorList1 = []\n",
    "    RewardList1 = []\n",
    "    totalReward1 = 0\n",
    "    EPSILON = epsilon\n",
    "    for episode in range(1):\n",
    "        env.reset()\n",
    "        # Process initial state and action\n",
    "        initialObservation = observationProcessor(initState, initAction)\n",
    "        # Get initial basis function and active indices \n",
    "        initialBasisFunction, initialActiveIndex = basisFunction(initialObservation)\n",
    "        #print('Initial Active Features:', initialActiveIndex)\n",
    "        # Initialize weights \n",
    "        WEIGHT_VECTOR = initializeWeights(initialBasisFunction)\n",
    "        #print('Len Initial Weight Vector:', len(WEIGHT_VECTOR))\n",
    "        # Divide into initial discovered and undiscovered feature sets\n",
    "        DISCOVERED, UNDISCOVERED = getInitialDiscoveredUndiscovered(initialActiveIndex)\n",
    "        print('Len Initial Discovered Set:', len(DISCOVERED), '\\nLen Initial Undiscovered Set:', len(UNDISCOVERED))\n",
    "        # Begin learning \n",
    "        oldObservation = initialObservation \n",
    "        # Multiply basis function and weights together\n",
    "        oldQValues = np.dot(initialBasisFunction, WEIGHT_VECTOR)\n",
    "        for timeSteps in range(episodeLen):\n",
    "            # Choose an action using epsilon-greedy\n",
    "            action, actionIndex = jointEpsilonGreedy(oldQValues)\n",
    "            # Get old Q value\n",
    "            oldQValue = 0.5 * (oldQValues[actionIndex[0]] + oldQValues[actionIndex[1]])\n",
    "            # Take a step in the environment \n",
    "            state, reward, done, info = env.step(action)\n",
    "            # Process observation\n",
    "            currentObservation = observationProcessor(state, action)\n",
    "            # Get basis function\n",
    "            currentBasisFunction, currentActiveIndex = basisFunction(currentObservation)\n",
    "            # Calculate new Q-values \n",
    "            currentQValues = np.dot(currentBasisFunction, WEIGHT_VECTOR)\n",
    "            # Update weights\n",
    "            WEIGHT_VECTOR, currentTDError = updateWeights(WEIGHT_VECTOR, \n",
    "                                          currentActiveIndex, \n",
    "                                          actionIndex, \n",
    "                                          reward, \n",
    "                                          oldQValue, \n",
    "                                          currentQValues)\n",
    "            # Logging \n",
    "            # Outputs information every 100000 timesteps\n",
    "            if timeSteps % 500000 == 0:\n",
    "                current_time = time.time()\n",
    "                print('Timestep:', timeSteps, \n",
    "                      'Time elapsed:', round(current_time - start_time, 2),\n",
    "                      'Cumulative Reward:', round(totalReward1, 4))\n",
    "\n",
    "            # Set current observation and Q-value to old\n",
    "            EPSILON += 0.00000001\n",
    "            totalReward1 += reward\n",
    "            TDErrorList1.append(currentTDError)\n",
    "            RewardList1.append(totalReward1)\n",
    "            oldObservation = currentObservation\n",
    "            oldQValues = currentQValues \n",
    "    end_time = time.time()\n",
    "    print('Training Time:', round(end_time - start_time, 2))\n",
    "    return TDErrorList1, RewardList1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For running Q-Learning w/ Sparse iFDD \n",
    "def QLearningIFDD(epsilon, episodeLen, myDiscoverFrequency):\n",
    "    start_time = time.time()\n",
    "    TDErrorList2 = []\n",
    "    RewardList2 = []\n",
    "    DiscoverTimeList = []\n",
    "    totalReward2 = 0\n",
    "    currentTDError = 0\n",
    "    EPSILON = epsilon\n",
    "    discoverFrequency = myDiscoverFrequency\n",
    "    global THRESHOLD\n",
    "    for episode in range(1):\n",
    "        env.reset()\n",
    "        # Start episode with random initial state and action   \n",
    "        initialState, initialAction = generateRandomStateActionPair()\n",
    "        # Process initial state and action\n",
    "        initialObservation = observationProcessor(initialState, initialAction)\n",
    "        # Get initial basis function and active indices \n",
    "        initialBasisFunction, initialActiveIndex = basisFunction(initialObservation)\n",
    "        print('Initial Active Features:', initialActiveIndex)\n",
    "        # Initialize ADDITIONAL_FEATURE_INDEX \n",
    "        ADDITIONAL_FEATURE_INDEX = len(initialBasisFunction)\n",
    "        # Initialize weights \n",
    "        WEIGHT_VECTOR = initializeWeights(initialBasisFunction)\n",
    "        print('Len Initial Weight Vector:', len(WEIGHT_VECTOR))\n",
    "        # Divide into initial discovered and undiscovered feature sets\n",
    "        DISCOVERED, UNDISCOVERED = getInitialDiscoveredUndiscovered(initialActiveIndex)\n",
    "        print('Len Initial Discovered Set:', len(DISCOVERED), '\\nLen Initial Undiscovered Set:', len(UNDISCOVERED))\n",
    "        # Begin learning \n",
    "        oldObservation = initialObservation \n",
    "        # Multiply basis function and weights together\n",
    "        oldQValues = np.dot(initialBasisFunction, WEIGHT_VECTOR)\n",
    "        currentBasisFunction = []\n",
    "        for timeSteps in range(episodeLen):\n",
    "            # Choose an action using epsilon-greedy\n",
    "            action, actionIndex = jointEpsilonGreedy(oldQValues)\n",
    "            # Get old Q value, which is the average of both Q-values\n",
    "            oldQValue = 0.5 * (oldQValues[actionIndex[0]] + oldQValues[actionIndex[1]])\n",
    "            # Take a step in the environment \n",
    "            state, reward, done, info = env.step(action)\n",
    "            # Process observation\n",
    "            currentObservation = observationProcessor(state, action)\n",
    "            # Get basis function - unmodified basis function for first time step only\n",
    "            unmodifiedBasisFunction, currentActiveIndex = basisFunction(currentObservation)\n",
    "            if timeSteps == 0:\n",
    "                currentBasisFunction = unmodifiedBasisFunction\n",
    "            else:\n",
    "                currentBasisFunction = currentBasisFunction\n",
    "\n",
    "            # Get modified basis function by inactivating subsets of conjunctions\n",
    "            currentBasisFunction = conjunctionBasisFunction(DISCOVERED, currentBasisFunction)\n",
    "            currentActiveIndex = [i for i, item in enumerate(currentBasisFunction) if item == 1]\n",
    "\n",
    "           # Calculate new Q-values \n",
    "            currentQValues = np.dot(currentBasisFunction, WEIGHT_VECTOR)\n",
    "            # Update weights using off-policy Q-learning\n",
    "            WEIGHT_VECTOR, currentTDError = updateWeights(WEIGHT_VECTOR, \n",
    "                                          currentActiveIndex, \n",
    "                                          actionIndex, \n",
    "                                          reward, \n",
    "                                          oldQValue, \n",
    "                                          currentQValues)\n",
    "\n",
    "            # Discover new features\n",
    "            PREVIOUS_DISCOVERED = DISCOVERED \n",
    "            if timeSteps % discoverFrequency == 0:\n",
    "                DISCOVERED, UNDISCOVERED, ADDITIONAL_FEATURE_INDEX = discover(currentTDError,\n",
    "                                                                            THRESHOLD,\n",
    "                                                                            DISCOVERED,\n",
    "                                                                            UNDISCOVERED,\n",
    "                                                                            ADDITIONAL_FEATURE_INDEX,\n",
    "                                                                            currentBasisFunction,\n",
    "                                                                            WEIGHT_VECTOR) \n",
    "                if len(PREVIOUS_DISCOVERED) < len(DISCOVERED):\n",
    "                    current_time = time.time()\n",
    "                    DiscoverTimeList.append(round(current_time - start_time, 2))\n",
    "                    discoverFrequency += discoverFrequency\n",
    "\n",
    "            # Logging\n",
    "            # Outputs information every 100000 timesteps\n",
    "            if timeSteps % 500000 == 0:\n",
    "                current_time = time.time()\n",
    "                print('Timestep:', timeSteps, \n",
    "                      'Elapsed:', round(current_time - start_time, 2),\n",
    "                      'DISCOVERED:', len(DISCOVERED),\n",
    "                      'UNDISCOVERED:', len(UNDISCOVERED), \n",
    "                      'Basis:', len(currentBasisFunction),\n",
    "                      'Weights:', len(WEIGHT_VECTOR),\n",
    "                      'Cumul. Reward:', round(totalReward2, 4))\n",
    "\n",
    "            # Set current observation and Q-value to old\n",
    "            EPSILON += 0.00000001\n",
    "            totalReward2 += reward\n",
    "            TDErrorList2.append(currentTDError)\n",
    "            RewardList2.append(totalReward2)\n",
    "            oldObservation = currentObservation\n",
    "            oldQValues = currentQValues\n",
    "            \n",
    "    end_time = time.time()\n",
    "    print('Training Time:', round(end_time - start_time, 2))\n",
    "    return TDErrorList2, RewardList2\n",
    "\n",
    "# For running Q-Learning w/ Sparse iFDD \n",
    "def QLearningIFDD2(epsilon, episodeLen, myDiscoverFrequency, initState, initAction):\n",
    "    start_time = time.time()\n",
    "    TDErrorList2 = []\n",
    "    RewardList2 = []\n",
    "    DiscoverTimeList = []\n",
    "    totalReward2 = 0\n",
    "    currentTDError = 0\n",
    "    EPSILON = epsilon\n",
    "    discoverFrequency = myDiscoverFrequency\n",
    "    global THRESHOLD\n",
    "    for episode in range(1):\n",
    "        env.reset()\n",
    "        # Process initial state and action\n",
    "        initialObservation = observationProcessor(initState, initAction)\n",
    "        # Get initial basis function and active indices \n",
    "        initialBasisFunction, initialActiveIndex = basisFunction(initialObservation)\n",
    "        #print('Initial Active Features:', initialActiveIndex)\n",
    "        # Initialize ADDITIONAL_FEATURE_INDEX \n",
    "        ADDITIONAL_FEATURE_INDEX = len(initialBasisFunction)\n",
    "        # Initialize weights \n",
    "        WEIGHT_VECTOR = initializeWeights(initialBasisFunction)\n",
    "        #print('Len Initial Weight Vector:', len(WEIGHT_VECTOR))\n",
    "        # Divide into initial discovered and undiscovered feature sets\n",
    "        DISCOVERED, UNDISCOVERED = getInitialDiscoveredUndiscovered(initialActiveIndex)\n",
    "        print('Len Initial Discovered Set:', len(DISCOVERED), '\\nLen Initial Undiscovered Set:', len(UNDISCOVERED))\n",
    "        # Begin learning \n",
    "        oldObservation = initialObservation \n",
    "        # Multiply basis function and weights together\n",
    "        oldQValues = np.dot(initialBasisFunction, WEIGHT_VECTOR)\n",
    "        currentBasisFunction = []\n",
    "        for timeSteps in range(episodeLen):\n",
    "            # Choose an action using epsilon-greedy\n",
    "            action, actionIndex = jointEpsilonGreedy(oldQValues)\n",
    "            # Get old Q value, which is the average of both Q-values\n",
    "            oldQValue = 0.5 * (oldQValues[actionIndex[0]] + oldQValues[actionIndex[1]])\n",
    "            # Take a step in the environment \n",
    "            state, reward, done, info = env.step(action)\n",
    "            # Process observation\n",
    "            currentObservation = observationProcessor(state, action)\n",
    "            # Get basis function - unmodified basis function for first time step only\n",
    "            unmodifiedBasisFunction, currentActiveIndex = basisFunction(currentObservation)\n",
    "            if timeSteps == 0:\n",
    "                currentBasisFunction = unmodifiedBasisFunction\n",
    "            else:\n",
    "                currentBasisFunction = currentBasisFunction\n",
    "\n",
    "            # Get modified basis function by inactivating subsets of conjunctions\n",
    "            currentBasisFunction = conjunctionBasisFunction(DISCOVERED, currentBasisFunction)\n",
    "            currentActiveIndex = [i for i, item in enumerate(currentBasisFunction) if item == 1]\n",
    "\n",
    "           # Calculate new Q-values \n",
    "            currentQValues = np.dot(currentBasisFunction, WEIGHT_VECTOR)\n",
    "            # Update weights using off-policy Q-learning\n",
    "            WEIGHT_VECTOR, currentTDError = updateWeights(WEIGHT_VECTOR, \n",
    "                                          currentActiveIndex, \n",
    "                                          actionIndex, \n",
    "                                          reward, \n",
    "                                          oldQValue, \n",
    "                                          currentQValues)\n",
    "\n",
    "            # Discover new features\n",
    "            PREVIOUS_DISCOVERED = DISCOVERED \n",
    "            if timeSteps % discoverFrequency == 0:\n",
    "                DISCOVERED, UNDISCOVERED, ADDITIONAL_FEATURE_INDEX = discover(currentTDError,\n",
    "                                                                            THRESHOLD,\n",
    "                                                                            DISCOVERED,\n",
    "                                                                            UNDISCOVERED,\n",
    "                                                                            ADDITIONAL_FEATURE_INDEX,\n",
    "                                                                            currentBasisFunction,\n",
    "                                                                            WEIGHT_VECTOR) \n",
    "                if len(PREVIOUS_DISCOVERED) < len(DISCOVERED):\n",
    "                    current_time = time.time()\n",
    "                    DiscoverTimeList.append(round(current_time - start_time, 2))\n",
    "                    discoverFrequency += discoverFrequency\n",
    "\n",
    "            # Logging\n",
    "            # Outputs information every 100000 timesteps\n",
    "            if timeSteps % 500000 == 0:\n",
    "                current_time = time.time()\n",
    "                print('Timestep:', timeSteps, \n",
    "                      'Elapsed:', round(current_time - start_time, 2),\n",
    "                      'DISCOVERED:', len(DISCOVERED),\n",
    "                      'UNDISCOVERED:', len(UNDISCOVERED), \n",
    "                      'Basis:', len(currentBasisFunction),\n",
    "                      'Weights:', len(WEIGHT_VECTOR),\n",
    "                      'Cumul. Reward:', round(totalReward2, 4))\n",
    "\n",
    "            # Set current observation and Q-value to old\n",
    "            EPSILON += 0.00000001\n",
    "            totalReward2 += reward\n",
    "            TDErrorList2.append(currentTDError)\n",
    "            RewardList2.append(totalReward2)\n",
    "            oldObservation = currentObservation\n",
    "            oldQValues = currentQValues\n",
    "            \n",
    "    end_time = time.time()\n",
    "    print('Training Time:', round(end_time - start_time, 2))\n",
    "    return TDErrorList2, RewardList2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batchQSiFDDTrain\n",
    "This method runs trials of Q-Learning vs. Q-Learning w/ Sparse iFDD as well as provides real-time training logs and a plot of the results at the end.\n",
    "- trials: number of trials\n",
    "- epsilon: e-greedy action selection parameter\n",
    "- episodeLen: number of timesteps to run\n",
    "- myDiscoverFrequency: frequency to start and get doubled after every discovery algorithm iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchQSiFDDTrain(trials, epsilon, episodeLen, myDiscoverFrequency):\n",
    "    print('-------BEGIN', trials, 'trials of Q-Learning vs. Q-Learning w/ Sparse iFDD for', episodeLen, 'episodes.-------')\n",
    "    episodeLen += 1\n",
    "    start_time = time.time()\n",
    "    for i in range(trials):\n",
    "        print('Trial:', i + 1)\n",
    "        # Initialize with same initial features for all experiments\n",
    "        initState, initAction = generateRandomStateActionPair()\n",
    "        initialObservation = observationProcessor(initState, initAction)\n",
    "        initialBasisFunction, initialActiveIndex = basisFunction(initialObservation)\n",
    "        print('Initial Active Features for Current Trial:', initialActiveIndex)\n",
    "        print('-----------Q-Learning-----------')\n",
    "        QLearnTD, QLearnReward = QLearning2(epsilon, episodeLen, initState, initAction)\n",
    "        print('-----------Q-Learning w/ Sparse iFDD-----------')\n",
    "        QLearniFDDTD, QLearniFDDReward = QLearningIFDD2(epsilon, episodeLen, myDiscoverFrequency, initState, initAction)\n",
    "        plt.plot(QLearnReward[1::10000], 'r--')\n",
    "        plt.plot(QLearniFDDReward[1::10000], 'b--')\n",
    "        current_time = time.time()\n",
    "        plt.title('Q-Learning w/o (red) vs. w/ Sparse iFDD (blue) for Initial Features:' + str(initialActiveIndex))\n",
    "        plt.xlabel('Time Steps (x 10^4)')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.show() \n",
    "        print('Time Elapsed:', round(current_time - start_time, 2))\n",
    "    # Show the plot\n",
    "    end_time = time.time()\n",
    "    print('Experiment complete!')\n",
    "    print('Total Training Time:', round((end_time - start_time)/3600, 2), 'hours')         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
