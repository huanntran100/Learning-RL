{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST Exercise\n",
    "I modify the MNIST tutorial on the tensorflow website to understand how graphs and sessions work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Tutorial\n",
    "From: https://www.tensorflow.org/get_started/mnist/beginners  \n",
    "MNIST data is basically a data set containing flattened [28, 28] images (so [None, 784]) of handwritten numbers from 0 to 9. There are 55,000 TRAINING images, 10,000 TEST images, and 5,000 VALIDATION images - very important to separate data into those three categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I copy the graph from the tutorial and write another simply graph. Whenever there's a placeholder, that's a place to use feed_dict to feed in external data into the graph to run the session later. I define all these operations inside a graph and call it g_1. I define another graph g_2 to practice working with multiple graphs at once.  \n",
    "There are basically a few basic elements defined inside a graph: **placeholders** (for feeding data into the graph), **variables** (the weights that you train), and **operations** (anything as simple as addition to the optimizer). In general, every session.run() statement consists of calling an **operation** and feeding data into the **placeholders**. Then the weights or **variables** are trained automatically. I'm sure it gets more complicated than this, but this is a start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph for training with MNIST data \n",
    "g_1 = tf.Graph()\n",
    "with g_1.as_default():   \n",
    "    # placeholder: x represents the flattened 28 x 28 image input  \n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "    # variable: W represents a 784 x 10 weight tensor that is variable during training \n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "    # variable: b represents a 1 x 10 bias tensor that is variable during training \n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    # operation: y is the output of this network\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    # placeholder: y_ represents the input for the correct data\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # operation: cross_entropy calculates the total loss \n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "    # operation: gradient descent - all those operations happen in here\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "    \n",
    "    # operation: determine correct predictions - supervised learning \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    \n",
    "    # operation: determine accuracy - supervised learning\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "# a random graph\n",
    "g_2 = tf.Graph()\n",
    "with g_2.as_default():\n",
    "    #placeholder \n",
    "    randomPlaceholder = tf.placeholder(tf.float32, [None, 2])\n",
    "    \n",
    "    # constant: fixed tensor \n",
    "    matrix1 = tf.constant([[1., 2.]])\n",
    "    \n",
    "    # constant: fixed tensor\n",
    "    matrix2 = tf.constant([[3.], [4.]])\n",
    "\n",
    "    # product1\n",
    "    product1 = tf.matmul(matrix1, matrix2)\n",
    "    \n",
    "    # product2\n",
    "    product2 = tf.matmul(randomPlaceholder, matrix2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can then define a tensorflow session within the \"with-as\" statement. All sess.run() calls must be run underneath inside the scope of the session. I use a .run() call to execute the training step on small batches of data. This automatically updates all the weights via SGD. If the network trains well, then the loss should decrease over the long run as shown below. The accuracy of about 0.90 means that the network successfully predicts the number from the image 90% of the time, assuming all the data is labeled correctly to begin with and that there are no false-positives or false-negatives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss at epoch 0 : 1.29822\n",
      "CE loss at epoch 100 : 0.249976\n",
      "CE loss at epoch 200 : 0.156431\n",
      "CE loss at epoch 300 : 0.332993\n",
      "CE loss at epoch 400 : 0.17977\n",
      "CE loss at epoch 500 : 0.240799\n",
      "CE loss at epoch 600 : 0.122391\n",
      "CE loss at epoch 700 : 0.284209\n",
      "CE loss at epoch 800 : 0.076709\n",
      "CE loss at epoch 900 : 0.123379\n",
      "MNIST g_1 graph accuracy: 0.912\n"
     ]
    }
   ],
   "source": [
    "# Do session with the MNIST graph \n",
    "with tf.Session(graph = g_1) as sess1: \n",
    "    tf.global_variables_initializer().run()\n",
    "    for _ in range(1000):\n",
    "        # Train in batches of 32\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(32)\n",
    "        # Do the train step operation on the batch we collect\n",
    "        # x and y are the placeholders \n",
    "        # we feed in train.images and train.labels \n",
    "        # train_step is the SGD operation from the graph\n",
    "        sess1.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "        # Use sess.run to call the accuracy operation by feeding in test images and test labels this time \n",
    "        if _ % 100 == 0:\n",
    "            print('CE loss at epoch', _, ':' , sess1.run(cross_entropy, feed_dict = {x: batch_xs, y_: batch_ys}))\n",
    "    print('MNIST g_1 graph accuracy:', sess1.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.]]\n",
      "[[ 46.]]\n"
     ]
    }
   ],
   "source": [
    "# Do a separate session on the random graph \n",
    "with tf.Session(graph = g_2) as sess2:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Running operation product1\n",
    "    print(sess2.run(product1))\n",
    "    # Running operation product2, which requires an input\n",
    "    print(sess2.run(product2, feed_dict = {randomPlaceholder: [[6., 7.]]}))\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
