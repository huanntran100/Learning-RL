{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient on CartPole-v0\n",
    "Based on: http://kvfrans.com/simple-algoritms-for-solving-cartpole/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "timeSteps = 200 # Time steps per episode\n",
    "observationSpace = 4 # Size of CartPole-v0 observation vector\n",
    "actionSpace = 2 # Size of CartPole-v0 action vector\n",
    "pglearningRate = 0.005 # policy gradient optimizer learning rate \n",
    "vglearningRate = 0.01 # value function optimizer learning rate \n",
    "numEpisodes = 3000 # Number of training episodes for value function - found experimentally\n",
    "discountFactor = 0.97 # For calculating discounted MC return \n",
    "batchSize = 420 # Batch size for gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-15 21:43:19,508] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on timestep: 11\n",
      "Example of action: 0\n",
      "Example of observation: [-0.14690084 -0.78639372  0.23155486  1.4895879 ]\n"
     ]
    }
   ],
   "source": [
    "# Setting up the CartPole-v0 environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done == True:\n",
    "        print('Done on timestep:', _)\n",
    "        print('Example of action:', action)\n",
    "        print('Example of observation:', observation)\n",
    "        break \n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_pg_parameters(env, parameters)** takes policy gradient weights and environment as input and returns total reward for nonlinear approximation. \n",
    "**run_10_pg_trials(env, parameters)** takes policy gradient weights and environment as input and prints the results of running the environment 100 times for a 3 trials with the weights.   \n",
    "**softmax(x)** is taken from here: https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_pg_parameters(env, parameters):\n",
    "    observation = env.reset()\n",
    "    totalReward = 0\n",
    "    for i in range(timeSteps):\n",
    "        # Choose action stochastically based on pg parameters\n",
    "        probabilityActionVector = softmax(np.matmul(observation, parameters))\n",
    "        randomNumber = np.random.uniform(0,1)\n",
    "        if randomNumber < probabilityActionVector[0]:\n",
    "            action = 0\n",
    "        else: \n",
    "            action = 1\n",
    "        # Take a step in the environment\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        # Append non-discounted cumulative reward \n",
    "        totalReward += reward \n",
    "        # If done, end the episode \n",
    "        if done:\n",
    "            break \n",
    "    return totalReward \n",
    "\n",
    "def run_10_pg_trials(env, parameters): \n",
    "    print('Using the following parameters: ', '\\n', parameters)\n",
    "    for i in range(10):\n",
    "        overallReward = []\n",
    "        for j in range(100):\n",
    "            episodeReward = run_pg_parameters(env, parameters)\n",
    "            overallReward.append(episodeReward)\n",
    "        print('Trial', i, ': The average reward over 100 consecutive trials is:', np.mean(overallReward), 'and the maximum reward is:', np.max(overallReward))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CartPole-v0, the policy is approximated as a neural network containing a set of weights shape [4,2] - 2 columns of weights corresponding to 2 actions, and 4 rows corresponding to the shape of the observations in the env. The policy recieves inputs of states and actions, and outputs the probability of choosing either action. Gradient ASCENT is used to update the weights to reflect better probabilities for choosing actions.   \n",
    "\n",
    "The value function is also approximated as a neural network containing a set of weights in a hidden layer for computing the value of a state (not a state-action pair, just the state). The advantage function is the difference between future return Monte-Carlo style and the computed value from the current network. Gradient DESCENT is then used to update the weights to reflect closer to actual values of the states.  \n",
    "\n",
    "Something to remember:   \n",
    "**Gradient Ascent** = optimizing negative of the loss (which is really positive)   \n",
    "**Gradient Descent** = optimizing positive of the loss (which is really negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Gradient Approximation\n",
    "policy_gradient = tf.Graph()\n",
    "with policy_gradient.as_default(): \n",
    "    # Inputs\n",
    "    pgstate = tf.placeholder(tf.float32, [None, observationSpace])\n",
    "    pgaction = tf.placeholder(tf.float32, [None, actionSpace])\n",
    "    pgadvantages = tf.placeholder(tf.float32,[None,1])\n",
    "    # Parameters \n",
    "    policyWeights = tf.get_variable(\"policyWeights\", [observationSpace, actionSpace])\n",
    "    # Output Operations \n",
    "    linear = tf.matmul(pgstate, policyWeights)\n",
    "    probabilities = tf.nn.softmax(linear) \n",
    "    # Output\n",
    "    processedProbabilities = tf.reduce_sum(tf.multiply(probabilities, pgaction), reduction_indices = [1])\n",
    "    # Gradient Ascent Operations\n",
    "    logProbabilities = tf.log(processedProbabilities)\n",
    "    eligibility = logProbabilities * pgadvantages \n",
    "    pgloss = (-1) * tf.reduce_sum(eligibility)\n",
    "    pgoptimizer = tf.train.RMSPropOptimizer(pglearningRate).minimize(pgloss)\n",
    "    \n",
    "# Value Function Approximation    \n",
    "value_function = tf.Graph()\n",
    "with value_function.as_default():\n",
    "    # Inputs \n",
    "    vlstate = tf.placeholder(tf.float32, [None, observationSpace])\n",
    "    # Input Operation\n",
    "    weight1 = tf.Variable(tf.zeros([4,10]))\n",
    "    bias1 = tf.Variable(tf.zeros([10]))\n",
    "    # Hidden Layer \n",
    "    hidden1 = tf.nn.relu(tf.matmul(vlstate, weight1) + bias1)\n",
    "    # Hidden Layer Operations\n",
    "    weight2 = tf.Variable(tf.zeros([10,1]))\n",
    "    bias2 = tf.Variable(tf.zeros([1]))\n",
    "    # Output Operations\n",
    "    calculated = tf.matmul(hidden1, weight2) + bias2\n",
    "    # Another input representing discounted MC return\n",
    "    newValues = tf.placeholder(tf.float32, [None,1])\n",
    "    # Output \n",
    "    difference = calculated - newValues\n",
    "    # Gradient Descent Operations\n",
    "    vlloss = tf.nn.l2_loss(difference)\n",
    "    vloptimizer = tf.train.RMSPropOptimizer(vglearningRate).minimize(vlloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we collect data from the environment using a random policy to run the episode to the terminal state. This is known as **Monte Carlo RL** because we randomly sample without bias from complete episodes in order to learn a model of the environment in the value function. These transitions will then be used to train the value function approximation.   \n",
    "The improvement of the value of the starting state is demonstrated below as verification of the value function training. Since the episodes end so soon due to a bad random policy, we have to increase the number of random policy rollouts. For my set-up, **3000 rollouts** allowed the value function to converge. I set this number emperically by seeing if the starting state asymptotically approached a set value, which was about **10.5** in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Starting state value: 0.03 Loss: 2558.52 Advantage: 19.97\n",
      "Episode 1000 Starting state value: 6.53 Loss: 497.73 Advantage: 11.24\n",
      "Episode 2000 Starting state value: 9.68 Loss: 212.39 Advantage: 6.07\n",
      "Episode 2999 Starting state value: 10.21 Loss: 1305.07 Advantage: 13.58\n",
      "Value function training time in seconds: 19.540735960006714\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "masterStatesList = []\n",
    "masterActionsList = []\n",
    "masterAdvantagesList = [] \n",
    "\n",
    "with tf.Session(graph = value_function) as sess1: \n",
    "    tf.global_variables_initializer().run()\n",
    "    # Sample state for value function learning verification\n",
    "    testObservation = env.reset()\n",
    "    # Run episodes of Cartpole to terminal state Monte-Carlo style   \n",
    "    for _ in range(numEpisodes):\n",
    "        observation = env.reset()\n",
    "        states = []\n",
    "        actions = []\n",
    "        transitions = []\n",
    "        advantages = [] \n",
    "        # Sample up to 200 random steps per episode or until terminal episode  \n",
    "        for t in range(timeSteps):\n",
    "            action = env.action_space.sample()\n",
    "            # Append state \n",
    "            masterStatesList.append(observation)\n",
    "            states.append(observation)\n",
    "            # Append action\n",
    "            actionVector = np.zeros(2)\n",
    "            actionVector[action] = 1\n",
    "            masterActionsList.append(actionVector)\n",
    "            actions.append(actionVector)\n",
    "            # Take a step in the environment\n",
    "            oldObservation = observation\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            # Record transitions\n",
    "            transitions.append([oldObservation, action, reward])\n",
    "            # Break if done with episode\n",
    "            if done == True:\n",
    "                break       \n",
    "        # Train the value function\n",
    "        updatedValues = []\n",
    "        for index, transitionTuples in enumerate(transitions):\n",
    "            observation, action, reward = transitionTuples\n",
    "            # Calculate discounted MC return starting from each state and moving towards terminal state\n",
    "            totalDiscountedReward = 0 \n",
    "            rolloutLength = len(transitions) - index \n",
    "            for i in range(rolloutLength):\n",
    "                # Calculate the discount factor\n",
    "                discount = np.power(discountFactor, i)\n",
    "                # transitions[i + index][2] is the third entry - [oldObservation, action, reward]\n",
    "                totalDiscountedReward += (transitions[i + index][2]) * discount \n",
    "            # Append the updated values to the list\n",
    "            updatedValues.append(totalDiscountedReward)\n",
    "            # Append the advantage to the list\n",
    "            currentValue = sess1.run(calculated, feed_dict = {vlstate: [observation]})\n",
    "            advantages.append(totalDiscountedReward - currentValue)\n",
    "            masterAdvantagesList.append(totalDiscountedReward - currentValue)\n",
    "        # Use the list to train the value function by calling sess2.run()\n",
    "        updatedValues = np.expand_dims(updatedValues, axis = 1)\n",
    "        sess1.run(vloptimizer, feed_dict = {vlstate: states, newValues: updatedValues})\n",
    "        # Training Log\n",
    "        if _ % 1000 == 0 or _ == numEpisodes - 1:\n",
    "            currentStateValue = sess1.run(calculated, feed_dict = {vlstate: [testObservation]})\n",
    "            currentLoss = sess1.run(vlloss, feed_dict = {vlstate: states, newValues: updatedValues})\n",
    "            print('Episode',_,'Starting state value:', round(float(currentStateValue), 2), 'Loss:', round(float(currentLoss), 2), 'Advantage:', round(float(advantages[0]), 2))        \n",
    "end_time = time.time()\n",
    "print('Value function training time in seconds:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we then train the policy gradient by training on batches from the state, action, and advantage data we collected earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Starting state chosen action: [[ 0.  1.]] probability: 0.54 Loss: 12.15\n",
      "Episode 10000 Starting state chosen action: [[ 0.  1.]] probability: 0.57 Loss: 11.03\n",
      "Episode 20000 Starting state chosen action: [[ 0.  1.]] probability: 0.59 Loss: 10.22\n",
      "Episode 30000 Starting state chosen action: [[ 0.  1.]] probability: 0.62 Loss: 9.47\n",
      "Episode 40000 Starting state chosen action: [[ 0.  1.]] probability: 0.64 Loss: 8.76\n",
      "Episode 50000 Starting state chosen action: [[ 0.  1.]] probability: 0.66 Loss: 8.12\n",
      "Episode 60000 Starting state chosen action: [[ 0.  1.]] probability: 0.68 Loss: 7.49\n",
      "Episode 67172 Starting state chosen action: [[ 0.  1.]] probability: 0.7 Loss: 7.07\n",
      "Policy gradient training time in seconds: 0.9735894203186035\n",
      "Untrained policy parameters: \n",
      " [[-0.25356054  0.23324466]\n",
      " [-0.38843489  0.17011738]\n",
      " [-0.68659639 -0.71196938]\n",
      " [-0.33538461  0.45692039]]\n",
      "Trained policy parameters: \n",
      " [[ 0.58406454 -0.60438049]\n",
      " [ 0.44919032 -0.66750789]\n",
      " [ 0.15102857 -1.54959464]\n",
      " [-1.17301035  1.29454553]]\n"
     ]
    }
   ],
   "source": [
    "# Helpful tips: http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_4_policy_gradient.pdf\n",
    "start_time = time.time()\n",
    "untrainedWeights = np.zeros((4,2))\n",
    "trainedWeights = np.zeros((4,2))\n",
    "with tf.Session(graph = policy_gradient) as sess2: \n",
    "    tf.global_variables_initializer().run()\n",
    "    untrainedWeights = sess2.run(policyWeights)\n",
    "    # Train policy gradient\n",
    "    stateBatch = []\n",
    "    actionBatch = []\n",
    "    advantageBatch = []\n",
    "    # Sample state for tracking the loss \n",
    "    testState = [masterStatesList[1]]\n",
    "    testAction = np.asarray(masterActionsList[1]).reshape(1, actionSpace)\n",
    "    testAdvantage = masterAdvantagesList[1]\n",
    "    # Start training\n",
    "    for _ in range(len(masterStatesList)):\n",
    "        # Batch training\n",
    "        if len(stateBatch) == batchSize: \n",
    "            stateBatchArray = np.asarray(stateBatch).reshape(batchSize, observationSpace)\n",
    "            actionBatchArray = np.asarray(actionBatch).reshape(batchSize, actionSpace)\n",
    "            advantageBatchArray = np.asarray(advantageBatch).reshape(batchSize, 1)\n",
    "            # print(sess1.run(linear, feed_dict = {pgstate: stateBatchArray, pgaction: actionBatchArray, pgadvantages: advantageBatchArray}))\n",
    "            sess2.run(pgoptimizer, feed_dict = {pgstate: stateBatchArray, pgaction: actionBatchArray, pgadvantages: advantageBatchArray})\n",
    "            # Empty batch\n",
    "            stateBatch = []\n",
    "            actionBatch = []\n",
    "            advantageBatch = []    \n",
    "        # Append to batch \n",
    "        else: \n",
    "            stateBatch.append([masterStatesList[i]])\n",
    "            actionBatch.append(np.asarray(masterActionsList[i]).reshape(1, actionSpace))\n",
    "            advantageBatch.append(masterAdvantagesList[i]) \n",
    "        # Training log\n",
    "        if _ % 10000 == 0 or _ == len(masterStatesList) - 1:\n",
    "            currentProbability = sess2.run(processedProbabilities, feed_dict = {pgstate: testState, pgaction: testAction, pgadvantages: testAdvantage})\n",
    "            currentLoss = sess2.run(pgloss, feed_dict = {pgstate: testState, pgaction: testAction, pgadvantages: testAdvantage})\n",
    "            print('Episode',_,'Starting state chosen action:', testAction,'probability:', round(float(currentProbability), 2), 'Loss:', round(float(currentLoss), 2))\n",
    "    trainedWeights = sess2.run(policyWeights)\n",
    "end_time = time.time()\n",
    "print('Policy gradient training time in seconds:', end_time - start_time)\n",
    "print('Untrained policy parameters:', '\\n', untrainedWeights)\n",
    "print('Trained policy parameters:','\\n' , trainedWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following parameters:  \n",
      " [[-0.25356054  0.23324466]\n",
      " [-0.38843489  0.17011738]\n",
      " [-0.68659639 -0.71196938]\n",
      " [-0.33538461  0.45692039]]\n",
      "Trial 0 : The average reward over 100 consecutive trials is: 25.0 and the maximum reward is: 93.0\n",
      "Trial 1 : The average reward over 100 consecutive trials is: 31.03 and the maximum reward is: 105.0\n",
      "Trial 2 : The average reward over 100 consecutive trials is: 31.18 and the maximum reward is: 92.0\n",
      "Trial 3 : The average reward over 100 consecutive trials is: 29.77 and the maximum reward is: 136.0\n",
      "Trial 4 : The average reward over 100 consecutive trials is: 29.58 and the maximum reward is: 79.0\n",
      "Trial 5 : The average reward over 100 consecutive trials is: 27.98 and the maximum reward is: 71.0\n",
      "Trial 6 : The average reward over 100 consecutive trials is: 29.57 and the maximum reward is: 88.0\n",
      "Trial 7 : The average reward over 100 consecutive trials is: 27.65 and the maximum reward is: 82.0\n",
      "Trial 8 : The average reward over 100 consecutive trials is: 29.86 and the maximum reward is: 83.0\n",
      "Trial 9 : The average reward over 100 consecutive trials is: 28.98 and the maximum reward is: 91.0\n"
     ]
    }
   ],
   "source": [
    "# Untrained weights\n",
    "run_10_pg_trials(env, untrainedWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following parameters:  \n",
      " [[ 0.58406454 -0.60438049]\n",
      " [ 0.44919032 -0.66750789]\n",
      " [ 0.15102857 -1.54959464]\n",
      " [-1.17301035  1.29454553]]\n",
      "Trial 0 : The average reward over 100 consecutive trials is: 56.22 and the maximum reward is: 131.0\n",
      "Trial 1 : The average reward over 100 consecutive trials is: 52.9 and the maximum reward is: 185.0\n",
      "Trial 2 : The average reward over 100 consecutive trials is: 50.39 and the maximum reward is: 124.0\n",
      "Trial 3 : The average reward over 100 consecutive trials is: 47.34 and the maximum reward is: 87.0\n",
      "Trial 4 : The average reward over 100 consecutive trials is: 54.38 and the maximum reward is: 142.0\n",
      "Trial 5 : The average reward over 100 consecutive trials is: 51.04 and the maximum reward is: 104.0\n",
      "Trial 6 : The average reward over 100 consecutive trials is: 50.89 and the maximum reward is: 135.0\n",
      "Trial 7 : The average reward over 100 consecutive trials is: 48.49 and the maximum reward is: 130.0\n",
      "Trial 8 : The average reward over 100 consecutive trials is: 56.82 and the maximum reward is: 165.0\n",
      "Trial 9 : The average reward over 100 consecutive trials is: 56.56 and the maximum reward is: 200.0\n"
     ]
    }
   ],
   "source": [
    "# Trained weights\n",
    "run_10_pg_trials(env, trainedWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Clearly the trained policy parameters perform better on average. But more hyperparameter tuning is needed for the learning rate, training batch sizes, and number of training episodes. I didn't solve the environment, but I can see why policy gradients are useful for continuous control as shown above. The tutorial also showed the random policy search actually performed the best due to the small action space. For more complicated environments like humanoid or swimmer, the policy gradient will be much more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
